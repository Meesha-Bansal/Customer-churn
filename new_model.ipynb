{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1739066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3771f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.best_score = 0\n",
    "        self.results = {}\n",
    "\n",
    "    def train_models(self, X_train, y_train, X_val, y_val):\n",
    "        model_grid = {\n",
    "            \"LogisticRegression\": {\n",
    "                \"model\": LogisticRegression(max_iter=1000),\n",
    "                \"params\": {\n",
    "                    \"C\": [0.01, 0.1, 1, 10],\n",
    "                    \"penalty\": ['l2'],\n",
    "                    \"solver\": ['lbfgs']\n",
    "                }\n",
    "            },\n",
    "            \"RandomForest\": {\n",
    "                \"model\": RandomForestClassifier(random_state=42),\n",
    "                \"params\": {\n",
    "                    \"n_estimators\": [100, 200],\n",
    "                    \"max_depth\": [None, 10, 20],\n",
    "                    \"min_samples_split\": [2, 5]\n",
    "                }\n",
    "            },\n",
    "            \"GradientBoosting\": {\n",
    "                \"model\": GradientBoostingClassifier(random_state=42),\n",
    "                \"params\": {\n",
    "                    \"n_estimators\": [100, 200],\n",
    "                    \"learning_rate\": [0.05, 0.1],\n",
    "                    \"max_depth\": [3, 5]\n",
    "                }\n",
    "            },\n",
    "            \"XGBoost\": {\n",
    "                \"model\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "                \"params\": {\n",
    "                    \"n_estimators\": [100, 200],\n",
    "                    \"learning_rate\": [0.05, 0.1],\n",
    "                    \"max_depth\": [3, 5]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for name, config in model_grid.items():\n",
    "            print(f\"\\nüîç Training and tuning {name}...\")\n",
    "            grid_search = GridSearchCV(config['model'], config['params'], cv=3, scoring='f1', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_pred = best_model.predict(X_val)\n",
    "\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "            self.results[name] = {\n",
    "                \"accuracy\": acc,\n",
    "                \"f1_score\": f1,\n",
    "                \"model\": best_model,\n",
    "                \"classification_report\": classification_report(y_val, y_pred, output_dict=True)\n",
    "            }\n",
    "\n",
    "            if f1 > self.best_score:\n",
    "                self.best_score = f1\n",
    "                self.best_model = best_model\n",
    "\n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def get_results(self):\n",
    "        return pd.DataFrame({\n",
    "            model: {\n",
    "                \"Accuracy\": round(result[\"accuracy\"], 4),\n",
    "                \"F1 Score\": round(result[\"f1_score\"], 4)\n",
    "            }\n",
    "            for model, result in self.results.items()\n",
    "        }).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058710e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Training and tuning LogisticRegression...\n",
      "\n",
      "üîç Training and tuning RandomForest...\n",
      "\n",
      "üîç Training and tuning GradientBoosting...\n",
      "\n",
      "üîç Training and tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\customerChurn\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:57:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Performance Summary:\n",
      "                    Accuracy  F1 Score\n",
      "LogisticRegression    0.8061    0.6029\n",
      "RandomForest          0.8107    0.6209\n",
      "GradientBoosting      0.8154    0.6232\n",
      "XGBoost               0.8224    0.6381\n",
      "üìÑ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88       937\n",
      "           1       0.71      0.58      0.64       347\n",
      "\n",
      "    accuracy                           0.82      1284\n",
      "   macro avg       0.78      0.75      0.76      1284\n",
      "weighted avg       0.82      0.82      0.82      1284\n",
      "\n",
      "üèÜ Best Model: XGBClassifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load your cleaned data\n",
    "df = pd.read_csv(\"../data/cleaned_churn_data.csv\")  # replace with your actual filename\n",
    "\n",
    "# 2. Separate features and target\n",
    "X = df.drop(columns=['churn'])\n",
    "y = df['churn']\n",
    "\n",
    "# 3. Split into train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "trainer = ModelTrainer()\n",
    "trainer.train_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"üìä Performance Summary:\")\n",
    "print(trainer.get_results())\n",
    "\n",
    "best_model = trainer.get_best_model()\n",
    "y_pred = best_model.predict(X_val)\n",
    "print(\"üìÑ Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"üèÜ Best Model:\", best_model.__class__.__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4251f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88       937\n",
      "           1       0.71      0.58      0.64       347\n",
      "\n",
      "    accuracy                           0.82      1284\n",
      "   macro avg       0.78      0.75      0.76      1284\n",
      "weighted avg       0.82      0.82      0.82      1284\n",
      "\n",
      "ROC AUC: 0.8617283684824032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_prob = best_model.predict_proba(X_val)[:,1]\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_val, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6035f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual  Predicted  Churn_Probability\n",
      "0       0          0           0.007882\n",
      "1       0          1           0.838810\n",
      "2       1          1           0.677160\n",
      "3       0          0           0.251933\n",
      "4       0          0           0.129099\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Get churn probabilities using predict_proba (class 1 = churn)\n",
    "y_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Optional: show few examples\n",
    "prob_df = pd.DataFrame({\n",
    "    \"Actual\": y_val.values,\n",
    "    \"Predicted\": y_pred,\n",
    "    \"Churn_Probability\": y_proba\n",
    "})\n",
    "print(prob_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f98f557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved with probability support\n"
     ]
    }
   ],
   "source": [
    "# Save model with churn probability support\n",
    "joblib.dump(best_model, \"../model/best_churn_model.pkl\")\n",
    "print(\"‚úÖ Model saved with probability support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf98cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
